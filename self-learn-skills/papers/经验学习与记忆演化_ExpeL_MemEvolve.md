# Agent 经验学习与记忆系统演化研究

## 论文概览

本文档介绍两篇关于 LLM Agent 自我进化能力的重要论文，它们从不同角度探索了 Agent 如何从经验中学习和成长。

| 论文 | 发表时间 | 会议/状态 | 核心贡献 |
|------|---------|----------|---------|
| **ExpeL** | 2023年8月 | AAAI-24 | 首个无需参数更新的经验学习框架 |
| **MemEvolve** | 2025年12月 | Preprint | 记忆架构本身的元演化 |

---

## 一、ExpeL: LLM Agents Are Experiential Learners

**论文**: [arXiv:2308.10144](https://arxiv.org/abs/2308.10144)  
**作者**: Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang  
**机构**: 清华大学等

### 1.1 问题背景

当前 LLM Agent 面临的困境：

| 挑战 | 说明 |
|------|------|
| **微调成本高** | 针对特定任务微调 LLM 需要大量计算资源 |
| **泛化能力下降** | 微调可能削弱模型的通用能力 |
| **API 模型无法微调** | GPT-4、Claude 等闭源模型仅通过 API 访问，参数不可修改 |

**核心问题**: 如何在**不更新模型参数**的情况下，让 Agent 从经验中学习？

### 1.2 ExpeL 框架

ExpeL (Experiential Learning) 的核心思想：**像人类一样从经验中学习**

```
┌────────────────────────────────────────────────────────────────────┐
│                      ExpeL 经验学习框架                             │
│                                                                    │
│   ┌─────────────────────────────────────────────────────────────┐ │
│   │                    训练阶段 (Experience Gathering)           │ │
│   │                                                             │ │
│   │   训练任务集                                                │ │
│   │       │                                                     │ │
│   │       ▼                                                     │ │
│   │   Agent 尝试任务 ──► 成功/失败轨迹                          │ │
│   │       │                                                     │ │
│   │       ▼                                                     │ │
│   │   ┌─────────────────────────────────────────────┐          │ │
│   │   │ 知识提取 (Knowledge Extraction)             │          │ │
│   │   │                                             │          │ │
│   │   │  成功轨迹 ──► 提取"正面经验"                │          │ │
│   │   │  失败轨迹 ──► 提取"教训/避免什么"           │          │ │
│   │   │                      │                      │          │ │
│   │   │                      ▼                      │          │ │
│   │   │              自然语言形式的洞见              │          │ │
│   │   │              (Insights in NL)               │          │ │
│   │   └─────────────────────────────────────────────┘          │ │
│   └─────────────────────────────────────────────────────────────┘ │
│                              │                                    │
│                              ▼                                    │
│   ┌─────────────────────────────────────────────────────────────┐ │
│   │                    推理阶段 (Inference)                     │ │
│   │                                                             │ │
│   │   新任务 ──► 检索相关洞见 + 相似轨迹                        │ │
│   │                    │                                        │ │
│   │                    ▼                                        │ │
│   │              注入 Prompt ──► 做出更好的决策                  │ │
│   └─────────────────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────────────────┘
```

### 1.3 关键机制

#### 1.3.1 对比学习 (Contrastive Learning)

ExpeL 的核心创新在于**对比成功与失败轨迹**：

```
成功轨迹: Task A ──► Action1 ──► Action2 ──► Action3 ──► ✓ Success
失败轨迹: Task A ──► Action1 ──► Action4 ──► Action5 ──► ✗ Failure
                                    ↑
                             关键分歧点
                                    │
                                    ▼
              提取洞见: "在 Task A 类型任务中，
                        应该选择 Action2 而不是 Action4，
                        因为 Action4 会导致..."
```

#### 1.3.2 洞见存储结构

| 组件 | 内容 | 用途 |
|------|------|------|
| **Insight Pool** | 从轨迹对比中提取的自然语言规则/经验 | 推理时检索相关规则 |
| **Trajectory Bank** | 历史成功轨迹的存储 | 作为 few-shot 示例 |

#### 1.3.3 推理时的经验利用

```
新任务输入
    │
    ├──► 语义检索 ──► 相关 Insights (规则/教训)
    │
    └──► 相似度匹配 ──► 相似成功轨迹 (few-shot 示例)
                │
                ▼
         组合进 Prompt ──► LLM 生成更优决策
```

### 1.4 实验结果

ExpeL 在多个基准测试上展示了显著的学习效果：

| 基准 | 任务类型 | ExpeL 提升 |
|------|---------|-----------|
| HotpotQA | 多跳问答 | 显著提升 |
| ALFWorld | 具身任务 | 持续改进 |
| WebShop | 网页交互 | 经验累积效果明显 |

**关键发现**:
- 随着经验积累，性能持续提升
- 提取的洞见具有跨任务迁移能力
- 无需任何参数更新即可实现学习

### 1.5 与其他方法的关系

```
┌─────────────────────────────────────────────────────────────────┐
│                    Agent 学习方法谱系                            │
│                                                                 │
│   参数更新方法                    无参数更新方法                  │
│   ┌─────────────┐               ┌─────────────────────────┐    │
│   │  Fine-tuning │               │  In-Context Learning    │    │
│   │  RLHF       │               │  ├─ Few-shot Prompting  │    │
│   │  SFT        │               │  ├─ Reflexion (反思)    │    │
│   └─────────────┘               │  └─ ExpeL (经验学习) ◄──│    │
│         │                       └─────────────────────────┘    │
│         │                                    │                  │
│         ▼                                    ▼                  │
│   需要模型权重                         仅需 API 访问             │
│   计算成本高                           成本低、灵活              │
└─────────────────────────────────────────────────────────────────┘
```

---

## 二、MemEvolve: Meta-Evolution of Agent Memory Systems

**论文**: [arXiv:2512.18746](https://arxiv.org/pdf/2512.18746)  
**作者**: OPPO AI Agent Team, LV-NUS Lab  
**机构**: OPPO, 新加坡国立大学

### 2.1 问题背景

现有 Agent 记忆系统的根本局限：

```
┌─────────────────────────────────────────────────────────────────┐
│              现有记忆系统的"静态性"问题                          │
│                                                                 │
│   任务 A (网页浏览)  ──► 固定记忆架构 ──► 效果好 ✓               │
│   任务 B (数学推理)  ──► 同一记忆架构 ──► 效果差 ✗               │
│   任务 C (代码生成)  ──► 同一记忆架构 ──► 效果一般               │
│                                                                 │
│   问题: 记忆架构是预定义的，无法适应不同任务的特点               │
└─────────────────────────────────────────────────────────────────┘
```

**核心洞见**: 不同任务需要不同的记忆策略，但现有系统的记忆架构是**静态**的。

### 2.2 人类学习类比

论文用人类学习者的类比来阐述其思想：

| 学习者类型 | 特征 | Agent 对应 |
|-----------|------|-----------|
| **平庸学习者** | 不从经验中学习 | 无记忆的 Agent |
| **熟练学习者** | 通过固定方式提取可复用技能 | 有固定记忆架构的 Agent (如 ExpeL) |
| **适应性学习者** | 动态调整学习策略本身 | **MemEvolve** |

```
┌─────────────────────────────────────────────────────────────────┐
│                       学习能力层次                               │
│                                                                 │
│   Level 1: 无记忆 Agent                                         │
│            └─ 每次任务从零开始                                   │
│                                                                 │
│   Level 2: 固定记忆 Agent (ExpeL, Voyager 等)                   │
│            └─ 积累经验，但记忆架构不变                           │
│                                                                 │
│   Level 3: 元演化记忆 Agent (MemEvolve) ◄── 本文贡献            │
│            └─ 不仅积累经验，还演化"如何记忆"的策略               │
└─────────────────────────────────────────────────────────────────┘
```

### 2.3 MemEvolve 框架

#### 2.3.1 双层演化结构

```
┌────────────────────────────────────────────────────────────────────┐
│                    MemEvolve 元演化框架                            │
│                                                                    │
│   ┌──────────────────────────────────────────────────────────────┐│
│   │              外层: 记忆架构演化 (Meta-Evolution)             ││
│   │                                                              ││
│   │   初始记忆架构 M₀                                            ││
│   │        │                                                     ││
│   │        ▼                                                     ││
│   │   评估性能 ──► 架构变异 ──► 选择最优 ──► M₁                  ││
│   │        │                                                     ││
│   │        ▼                                                     ││
│   │   评估性能 ──► 架构变异 ──► 选择最优 ──► M₂                  ││
│   │        │                                                     ││
│   │        ▼                                                     ││
│   │       ...                                                    ││
│   └──────────────────────────────────────────────────────────────┘│
│                              │                                    │
│                              ▼                                    │
│   ┌──────────────────────────────────────────────────────────────┐│
│   │              内层: 经验知识积累 (Experience Evolution)       ││
│   │                                                              ││
│   │   使用当前记忆架构 Mₙ                                        ││
│   │        │                                                     ││
│   │        ▼                                                     ││
│   │   执行任务 ──► 收集轨迹 ──► 存储/检索/管理经验               ││
│   └──────────────────────────────────────────────────────────────┘│
└────────────────────────────────────────────────────────────────────┘
```

#### 2.3.2 EvolveLab: 统一记忆系统代码库

论文同时提出了 **EvolveLab**，将 12 个代表性记忆系统统一到模块化设计空间：

```
┌────────────────────────────────────────────────────────────────────┐
│                    EvolveLab 模块化设计空间                        │
│                                                                    │
│   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌────────┐│
│   │   Encode    │   │    Store    │   │  Retrieve   │   │ Manage ││
│   │   (编码)    │   │   (存储)    │   │   (检索)    │   │ (管理) ││
│   └──────┬──────┘   └──────┬──────┘   └──────┬──────┘   └───┬────┘│
│          │                 │                 │               │     │
│          ▼                 ▼                 ▼               ▼     │
│   • 原始轨迹        • 向量数据库      • 语义相似度     • 淘汰策略 │
│   • 摘要提取        • 结构化存储      • 时间相关性     • 合并去重 │
│   • 工具/技能       • 代码仓库        • 混合检索       • 老化机制 │
│   • 反思洞见        • 图结构          • 阶段感知       • 冲突解决 │
│   └─────────────────────────────────────────────────────────────┘  │
│                                                                    │
│   12个已实现的记忆系统:                                            │
│   ExpeL, SkillWeaver, AgentKB, MemoryBank, Voyager, Reflexion...   │
└────────────────────────────────────────────────────────────────────┘
```

#### 2.3.3 记忆架构的演化操作

MemEvolve 通过以下操作演化记忆架构：

| 操作类型 | 说明 | 示例 |
|---------|------|------|
| **编码策略变异** | 改变如何从轨迹中提取信息 | 从"存储原始轨迹"变为"提取高层洞见" |
| **存储结构变异** | 改变信息的组织方式 | 从"向量库"变为"分层知识图谱" |
| **检索策略变异** | 改变如何查找相关记忆 | 从"语义相似度"变为"阶段感知检索" |
| **管理策略变异** | 改变记忆的生命周期管理 | 添加"工作记忆维护机制" |

### 2.4 演化出的代表性记忆系统

论文展示了通过 MemEvolve 演化出的三种典型记忆架构：

#### LIGHTWEIGHT
```
起点: 简单的 few-shot 轨迹记忆 (类似 MemoryBank)
      │
      ▼ (演化)
终点: 结构化的阶段感知记忆系统
      • 按任务阶段组织记忆
      • 更精细的检索策略
```

#### RIVA
```
起点: AgentKB 风格架构 (无离线知识库)
      │
      ▼ (演化)
终点: Agent 中心的编码和检索策略
      • 更轻量级
      • 完全在线运行
```

#### CEREBRA
```
起点: AgentKB 风格架构
      │
      ▼ (演化)
终点: 复合记忆系统
      • 同时蒸馏可复用工具 + 抽象知识
      • 包含工作记忆维护机制
      • 支持长期 Agent 演化
```

### 2.5 实验结果

在四个挑战性基准测试上的表现：

| 基准 | 任务类型 | MemEvolve 提升 |
|------|---------|---------------|
| **GAIA** | 综合 Agent 任务 | 最高 +17.06% |
| **WebWalkerQA** | 复杂网页交互 | 显著提升 |
| **xBench-DeepSearch** | 规划+工具使用+推理 | 跨任务泛化好 |
| **TaskCraft** | 合成基准 | 跨 LLM 泛化好 |

**关键发现**:
1. **显著性能提升**: 在 SmolAgent 和 Flash-Searcher 框架上提升最高达 17.06%
2. **跨任务泛化**: 演化出的记忆架构可迁移到不同任务
3. **跨 LLM 泛化**: 演化出的架构可用于不同的底层模型

---

## 三、两篇论文的关系与对比

### 3.1 演化层次对比

```
┌─────────────────────────────────────────────────────────────────┐
│                    Agent 自我演化的层次                          │
│                                                                 │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  Level 3: 记忆架构演化 (MemEvolve)                      │  │
│   │           "如何记忆"的策略本身在演化                     │  │
│   │  ┌─────────────────────────────────────────────────┐   │  │
│   │  │  Level 2: 经验知识演化 (ExpeL)                   │   │  │
│   │  │           积累洞见、轨迹、技能                    │   │  │
│   │  │  ┌─────────────────────────────────────────┐    │   │  │
│   │  │  │  Level 1: 任务执行                       │    │   │  │
│   │  │  │           接收任务、采取行动、获得结果    │    │   │  │
│   │  │  └─────────────────────────────────────────┘    │   │  │
│   │  └─────────────────────────────────────────────────┘   │  │
│   └─────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 技术特点对比

| 维度 | ExpeL | MemEvolve |
|------|-------|-----------|
| **演化对象** | 经验知识 (洞见+轨迹) | 经验知识 + 记忆架构 |
| **记忆架构** | 固定 (Insight Pool + Trajectory Bank) | 可演化的模块化设计 |
| **学习机制** | 对比学习 (成功 vs 失败) | 元演化 (架构变异+选择) |
| **适应性** | 任务级适应 | 任务类型级适应 |
| **开销** | 低（仅 Prompt 工程） | 中（需要架构搜索） |
| **参数更新** | 无需 | 无需 |

### 3.3 互补关系

```
┌─────────────────────────────────────────────────────────────────┐
│                    ExpeL 与 MemEvolve 的关系                    │
│                                                                 │
│   ExpeL 可以作为 MemEvolve 的一个"记忆系统实例"                 │
│                                                                 │
│   MemEvolve 的 EvolveLab 中已实现 ExpeL:                        │
│   ┌─────────────────────────────────────────────────────────┐  │
│   │  ExpeLProvider:                                         │  │
│   │  • take_in_memory: 对比成功/失败轨迹 → 提取洞见         │  │
│   │  • provide_memory: 语义相似度检索 → 返回相关洞见        │  │
│   └─────────────────────────────────────────────────────────┘  │
│                                                                 │
│   MemEvolve 可以从 ExpeL 出发，演化出更好的记忆架构             │
└─────────────────────────────────────────────────────────────────┘
```

---

## 四、与上下文压缩研究的关联

这两篇论文与"主动上下文压缩"研究有密切关联：

| 关联点 | ExpeL | MemEvolve |
|--------|-------|-----------|
| **信息蒸馏** | 从轨迹中提取高层洞见 = 压缩原始交互历史 | 记忆编码策略 = 压缩策略 |
| **选择性保留** | 保留"有价值"的洞见和示例 | 记忆管理策略 = 淘汰/合并策略 |
| **检索增强** | 按需检索相关经验 | 可演化的检索策略 |
| **上下文效率** | 用洞见替代完整轨迹 | 演化出更高效的记忆表示 |

**与 Focus Agent 的对比**:
- Focus Agent: 在**单次任务内**主动压缩上下文
- ExpeL/MemEvolve: 在**跨任务**层面积累和演化知识

---

## 五、实践启示

### 5.1 方法选择指南

| 场景 | 推荐方法 | 理由 |
|------|---------|------|
| **固定任务类型，快速部署** | ExpeL | 简单有效，无需架构搜索 |
| **多样化任务，长期运行** | MemEvolve | 架构可适应不同任务特点 |
| **资源受限** | ExpeL | 开销更低 |
| **追求最优性能** | MemEvolve | 可发现任务特定的最优架构 |

### 5.2 与现有方法的结合

```
┌─────────────────────────────────────────────────────────────────┐
│                    组合使用建议                                  │
│                                                                 │
│   Focus Agent (单任务内压缩)                                    │
│        +                                                        │
│   ExpeL (跨任务经验积累)                                        │
│        +                                                        │
│   MemEvolve (记忆架构适应)                                      │
│        ↓                                                        │
│   = 多层次自适应的 Agent 系统                                    │
└─────────────────────────────────────────────────────────────────┘
```

---

## 六、资源链接

| 论文 | arXiv | 代码 |
|------|-------|------|
| ExpeL | [2308.10144](https://arxiv.org/abs/2308.10144) | - |
| MemEvolve | [2512.18746](https://arxiv.org/pdf/2512.18746) | [GitHub](https://github.com/bingreeky/MemEvolve) |

---

## 七、总结

**ExpeL** 证明了 LLM Agent 可以在**不更新参数**的情况下通过经验学习持续提升，其核心是通过对比成功/失败轨迹提取可复用的自然语言洞见。

**MemEvolve** 更进一步，提出记忆系统本身也应该演化——不仅积累"记什么"，还优化"怎么记"。这种元演化思想使 Agent 能够针对不同任务类型自动发现最优的记忆架构。

两篇论文共同指向一个方向：**Agent 的自我进化不仅是知识的积累，更是学习策略本身的优化**。这与 self-learn-skills 项目的 experience-crystallizer 技能有着相似的理念——将临时的试错经验转化为可复用的持久知识。


